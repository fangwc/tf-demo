{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainDF = pd.read_csv(\"./Titanic/train.csv\")\n",
    "testDF = pd.read_csv(\"./Titanic/test.csv\")\n",
    "feature_columns = 'Sex Age SibSp Parch Ticket Fare Cabin Embarked'.split()\n",
    "trainDF.sample(5)\n",
    "x_trainDF, x_validationDF, y_trainDF, y_validationDF = train_test_split(trainDF[feature_columns], trainDF['Survived'], test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data process\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from itertools import product\n",
    "\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    "\n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    "\n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    "\n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    "\n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    "\n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    "\n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    "\n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({'mean': 'mean', 'beta': 'size'})\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n",
    "\n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    "\n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "\n",
    "        return X_new\n",
    "\n",
    "\n",
    "class oneHotEncoder:\n",
    "    \n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_variance(p):\n",
    "        return p * (1 - p)\n",
    "    \n",
    "    def dum_sign(self, df, col, threshold=0.01):\n",
    "        dummy_col = df[col].fillna('')\n",
    "        dummy_col = dummy_col.astype(str)\n",
    "        p = dummy_col.value_counts() / dummy_col.shape[0]\n",
    "        mask = dummy_col.isin(p[self.binary_variance(p) >= threshold].index)\n",
    "        dummy_col[~mask] = np.nan\n",
    "        res = pd.get_dummies(dummy_col, prefix=col, dummy_na=False, sparse = False)\n",
    "        return res\n",
    "    \n",
    "    def one_hot_encoding(self, X, threshold):\n",
    "        dfs = []\n",
    "        for col in X.columns:\n",
    "            if type(threshold) == float:\n",
    "                t = threshold\n",
    "            elif col in threshold:\n",
    "                t = threshold[col]\n",
    "            else:\n",
    "                t = 0.0\n",
    "            df = self.dum_sign(X, col, t)\n",
    "            dfs.append(df)\n",
    "        res = pd.concat(dfs, axis=1)\n",
    "        return res\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        res = self.one_hot_encoding(df, self.threshold)\n",
    "        self.columns = res.columns\n",
    "        return res\n",
    "    \n",
    "    def transform(self, df):\n",
    "        res = self.one_hot_encoding(df, 0.0)\n",
    "        return res.reindex(columns = self.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = oneHotEncoder(0.002)\n",
    "trainX = c.fit_transform(x_trainDF)\n",
    "validataionX = c.transform(x_validationDF)\n",
    "trainY = y_trainDF\n",
    "validataionY = y_validationDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Init \n",
    "feature_size = trainX.shape[1]\n",
    "class_size = 2\n",
    "learning_rate = 0.1\n",
    "training_epochs = 1000\n",
    "batch_size = 32\n",
    "display_step = 50\n",
    "logs_path = \"/Users/sam/workspace/code/tf-demo/tensorboard_log\"\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, feature_size])\n",
    "y = tf.placeholder(tf.float32, [None, class_size])\n",
    "\n",
    "#Set model weights\n",
    "W = tf.Variable(tf.zeros([feature_size, class_size]))\n",
    "b = tf.Variable(tf.zeros([class_size]))\n",
    "\n",
    "#construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W)+b)\n",
    "\n",
    "#Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_mean(y*tf.log(pred), reduction_indices = 1))\n",
    "\n",
    "#Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "#auc\n",
    "auc_op, auc= tf.metrics.auc(y, pred)\n",
    "\n",
    "#Initialize the variables \n",
    "# init = tf.global_variables_initializer()\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer(), tf.initialize_all_variables())\n",
    "\n",
    "#tf.summary\n",
    "tf.summary.scalar('cost', cost)\n",
    "tf.summary.scalar('auc', auc)\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nextbatch(X, Y, batch, batch_size):\n",
    "    batch_x = X[batch*batch_size:(batch+1)*batch_size].values\n",
    "    batch_y = Y[batch*batch_size:(batch+1)*batch_size].values\n",
    "    batch_y = [[i, 1-i] for i in batch_y]\n",
    "    return batch_x, batch_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 0.208158092 0.87405694 [0.8741463]\n",
      "Epoch: 0100 cost= 0.191077776 0.8907536 [0.890775]\n",
      "Epoch: 0150 cost= 0.179655975 0.901127 [0.90112793]\n",
      "Epoch: 0200 cost= 0.171356999 0.9085742 [0.9085672]\n",
      "Epoch: 0250 cost= 0.164963420 0.9142791 [0.9142691]\n",
      "Epoch: 0300 cost= 0.159830507 0.9188223 [0.9188109]\n",
      "Epoch: 0350 cost= 0.155587102 0.92256695 [0.92255473]\n",
      "Epoch: 0400 cost= 0.152002643 0.9257055 [0.9256931]\n",
      "Epoch: 0450 cost= 0.148924483 0.92838305 [0.92837065]\n",
      "Epoch: 0500 cost= 0.146246232 0.93071616 [0.9307039]\n",
      "Epoch: 0550 cost= 0.143890694 0.9327563 [0.9327443]\n",
      "Epoch: 0600 cost= 0.141800078 0.9345597 [0.9345479]\n",
      "Epoch: 0650 cost= 0.139930029 0.9361751 [0.9361638]\n",
      "Epoch: 0700 cost= 0.138245808 0.9376215 [0.93761057]\n",
      "Epoch: 0750 cost= 0.136719774 0.9389407 [0.93893015]\n",
      "Epoch: 0800 cost= 0.135329603 0.9401386 [0.9401282]\n",
      "Epoch: 0850 cost= 0.134057082 0.9412407 [0.9412308]\n",
      "Epoch: 0900 cost= 0.132887156 0.94225407 [0.9422445]\n",
      "Epoch: 0950 cost= 0.131807299 0.94319355 [0.9431843]\n",
      "Epoch: 1000 cost= 0.130806972 0.9440653 [0.94405633]\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(trainX.shape[0]/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = nextbatch(trainX, trainY, i, batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c, auc_res, summary = sess.run([optimizer, cost, auc, merged_summary_op], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            validataion_xs, validataion_ys = nextbatch(validataionX, validataionY, 0, validataionX.shape[0])\n",
    "            auc_test = sess.run([auc], feed_dict={x: validataion_xs, y: validataion_ys})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost), auc_res, auc_test)\n",
    "            \n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
